{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acfcb0c2-5f1b-4efb-aabe-a413655607ac",
   "metadata": {},
   "source": [
    "## Que 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af1763-b667-4573-810f-28dc24d86d35",
   "metadata": {},
   "source": [
    "**Overfitting**: Model memorizes data, performs well on training, poorly on new data.\n",
    "\n",
    "**Underfitting**: Model is too simple, performs poorly on both training and test data.\n",
    "\n",
    "**Consequences**: Overfitting - poor generalization. Underfitting - can't learn patterns.\n",
    "\n",
    "**Mitigation (Overfitting)**: Simplify, Regularize, Cross-Validation, Early Stopping, Feature Selection, Ensembles.\n",
    "\n",
    "**Mitigation (Underfitting)**: Feature Engineering, Complex Models, Hyperparameter Tuning, Adding Features, Ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc7d11-adbd-48cf-9643-24d5b7a86864",
   "metadata": {},
   "source": [
    "## Que 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c96f44-ac74-454a-b4da-7cbc2d7cb563",
   "metadata": {},
   "source": [
    "Reduce overfitting by:\n",
    "1. **Simplifying Model**: Use simpler algorithms or fewer features.\n",
    "2. **Regularization**: Penalize complex models (Ridge, Lasso regression).\n",
    "3. **Cross-Validation**: Evaluate on various train-test splits.\n",
    "4. **Early Stopping**: Stop training when validation error increases.\n",
    "5. **Feature Selection**: Keep only relevant features, remove noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51185969-b7fc-4185-8e8c-f3bd609d94f0",
   "metadata": {},
   "source": [
    "## Que 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af1d67-9929-4ec7-8d63-af10c4befa56",
   "metadata": {},
   "source": [
    "Underfitting occurs when a model is too simple to capture underlying patterns in data, resulting in poor performance on both training and test data.\n",
    "\n",
    "Scenarios of underfitting in ML:\n",
    "1. **Insufficient Complexity**: Using linear model for highly nonlinear data.\n",
    "2. **Few Features**: When too few relevant features are used.\n",
    "3. **Ignoring Interactions**: Not considering interactions between features.\n",
    "4. **Too Much Regularization**: Excessive regularization in models like Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a6d41-4fa9-4522-a6a2-d17ac278137c",
   "metadata": {},
   "source": [
    "## Que 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3010771-e7d9-46ea-888e-6dcb507d5654",
   "metadata": {},
   "source": [
    "**Bias-Variance Tradeoff**: Balancing model simplicity (bias) and complexity (variance) for optimal performance.\n",
    "\n",
    "**Bias**: Error from simple assumptions, leads to underfitting.\n",
    "\n",
    "**Variance**: Error from sensitivity to data variations, causes overfitting.\n",
    "\n",
    "**High Bias**: Underfitting, poor on training and test.\n",
    "\n",
    "**High Variance**: Overfitting, good on training, poor on test.\n",
    "\n",
    "**Optimal Tradeoff**: Balancing bias and variance for best generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b865c80-8597-416f-90ae-72a93a0120ba",
   "metadata": {},
   "source": [
    "## Que 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e92ab8-7fbe-47d8-9b27-431ed317ed37",
   "metadata": {},
   "source": [
    "**Detecting Overfitting**:\n",
    "1. **Validation Curves**: Plotting training and validation error against model complexity.\n",
    "2. **Learning Curves**: Plotting training and validation error against training data size.\n",
    "3. **Cross-Validation**: Evaluating model on multiple train-test splits.\n",
    "4. **Comparing Train and Test Error**: If train error is much lower than test error, it's likely overfitting.\n",
    "\n",
    "**Detecting Underfitting**:\n",
    "1. **Training and Test Error**: If both errors are high, the model might be too simple.\n",
    "2. **Visual Inspection**: Plotting data and model predictions can reveal underfitting patterns.\n",
    "3. **Learning Curves**: If both training and validation error are high and close, it indicates underfitting.\n",
    "\n",
    "**Determining Overfitting/Underfitting**:\n",
    "- If training error is low, but validation/test error is high, it's likely overfitting.\n",
    "- If both training and validation/test error are high, it's likely underfitting.\n",
    "- A balanced model has reasonably low training and validation/test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc2304-2424-454c-8bbf-cf7ea7b9fc2e",
   "metadata": {},
   "source": [
    "## Que 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eccf7b-9cbd-457f-94df-39fd172d93a5",
   "metadata": {},
   "source": [
    "**Bias**:\n",
    "- **Definition**: Error from simplistic assumptions.\n",
    "- **Effect**: Underfitting, poor on train and test.\n",
    "- **Example**: Linear regression on complex data.\n",
    "\n",
    "**Variance**:\n",
    "- **Definition**: Error from data sensitivity.\n",
    "- **Effect**: Overfitting, good on train, poor on test.\n",
    "- **Example**: High-degree polynomial regression.\n",
    "\n",
    "**High Bias Model**:\n",
    "- Linear regression on nonlinear data.\n",
    "- Performance: Poor on both.\n",
    "\n",
    "**High Variance Model**:\n",
    "- High-degree polynomial regression on limited data.\n",
    "- Performance: Good on train, poor on test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771901e1-682b-4ee9-a21e-101f899a6ea0",
   "metadata": {},
   "source": [
    "## Que 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffddf1dc-5e09-4f81-aeaa-042882c1ea0a",
   "metadata": {},
   "source": [
    "**Regularization**: Adding a penalty term to the loss function to control model complexity and prevent overfitting.\n",
    "\n",
    "**Usage for Preventing Overfitting**:\n",
    "- **High Model Complexity**: Regularization discourages overly complex models.\n",
    "- **Reduces Overfitting**: Penalty on complexity makes model generalize better.\n",
    "\n",
    "**Common Regularization Techniques**:\n",
    "1. **L1 Regularization (Lasso)**:\n",
    "   - Adds the absolute values of coefficients to the loss.\n",
    "   - Encourages sparsity, some coefficients become exactly zero.\n",
    "   \n",
    "2. **L2 Regularization (Ridge)**:\n",
    "   - Adds the squared values of coefficients to the loss.\n",
    "   - Penalizes large coefficients, but doesn't make them exactly zero.\n",
    "   \n",
    "3. **Elastic Net Regularization**:\n",
    "   - Combination of L1 and L2 regularization.\n",
    "   - Balances between sparsity (L1) and coefficient size control (L2).\n",
    "\n",
    "4. **Dropout**:\n",
    "   - Neural network technique where random neurons are ignored during training.\n",
    "   - Helps prevent co-adaptation of neurons, reducing overfitting.\n",
    "\n",
    "5. **Early Stopping**:\n",
    "   - Stop training when validation error starts increasing.\n",
    "   - Prevents overfitting as model becomes too specialized to training data.\n",
    "\n",
    "Regularization techniques constrain the model's freedom to fit the training data too closely, helping it generalize better to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
